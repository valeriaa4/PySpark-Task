{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMalf/Khpk/a3wrerbIYXwa"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install pyspark boto3\n",
        "! pip install -q awscli\n",
        "! aws configure"
      ],
      "metadata": {
        "id": "wIR8kqBTpm23",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import boto3\n",
        "import json\n",
        "from boto3.dynamodb.conditions import Key\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from datetime import datetime, timedelta\n",
        "from dateutil.relativedelta import relativedelta\n",
        "\n",
        "spark = SparkSession.builder.appName(\"PySpark2\").getOrCreate()\n",
        "\n",
        "dynamodb = boto3.resource('dynamodb')\n",
        "table = dynamodb.Table('MARKET_LIST')\n",
        "\n",
        "response = table.query(\n",
        "    KeyConditionExpression=Key('PK').eq('USER#b4c894d8-3091-70b2-79f6-78ebfd1b527f')\n",
        ")\n",
        "data = response['Items']\n",
        "\n",
        "pdf = pd.DataFrame(data)\n",
        "schema = StructType([StructField(col, StringType(), True) for col in pdf.columns])\n",
        "df = spark.createDataFrame(pdf, schema)\n",
        "\n",
        "# filtro Ãºltimos 6 meses\n",
        "hoje = datetime.now()\n",
        "limite = hoje - timedelta(days=6*30)\n",
        "df = df[df['date'] >= limite]\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "id": "GPATBnZgjxEp",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#filtra e lista as tarefas\n",
        "df = df.filter(\n",
        "    ((df.type_task == \"Tarefa a Ser Feita\") & (df.status == \"todo\") & (df.date < date_sub(current_date(), 15))) |\n",
        "    ((df.type_task == \"Item de Compra\") & (df.status == \"todo\") & (df.date < date_sub(current_date(), 30)))\n",
        ")\n",
        "\n",
        "df = df.withColumn(\"month\", date_format(\"date\", \"yyyy-MM\"))\n",
        "df.select(\"month\", \"date\", \"type_task\", \"name\", \"status\", \"PK\", \"SK\").orderBy(\"month\", \"date\").show(truncate=False)"
      ],
      "metadata": {
        "id": "1TT4U9kB0UWM",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# formatar meses\n",
        "months = []\n",
        "for i in range(5, -1, -1):\n",
        "    months.append((hoje - relativedelta(months=i)).strftime(\"%Y-%m\"))\n",
        "df_months = spark.createDataFrame([(m,) for m in months], [\"month\"])\n",
        "\n",
        "df = df_months.join(df, on=\"month\", how=\"left\")\n",
        "\n",
        "# formatar planilha\n",
        "df = (\n",
        "    df.groupBy(\"month\")\n",
        "      .pivot(\"type_task\", [\"Tarefa a Ser Feita\", \"Item de Compra\"])\n",
        "      .agg(count(\"*\"))\n",
        "      .na.fill(0)\n",
        "      .orderBy(\"month\")\n",
        ")\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "id": "EqFVdyRUrVoh",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gerar .csv\n",
        "df = df.toPandas()\n",
        "df.to_csv(\"abandoned_tasks.csv\", index=False)"
      ],
      "metadata": {
        "id": "oB6A-jzK1Rqd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}